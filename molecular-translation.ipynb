{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bryan194/molecular-translation?scriptVersionId=100061047\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"### References:\n\n- [starter notebook from Y. Nakama](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)\n- [adapted notebook from Konrad](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)\n- [PyTorch tutorial on image captioning](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)\n- [two-layer RNN implementation](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/pull/79)","metadata":{}},{"cell_type":"code","source":"import os\nfrom matplotlib import pyplot as plt\n\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n\nimport os\nimport gc\nimport re\nimport math\nimport time\nimport random\nimport shutil\nimport pickle\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport Levenshtein\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nfrom albumentations import (\n    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n    IAAAdditiveGaussianNoise, Transpose, Blur\n    )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport timm\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"papermill":{"duration":5.105933,"end_time":"2021-04-07T08:27:02.380708","exception":false,"start_time":"2021-04-07T08:26:57.274775","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:18.692535Z","iopub.execute_input":"2022-07-01T06:07:18.693085Z","iopub.status.idle":"2022-07-01T06:07:20.544558Z","shell.execute_reply.started":"2022-07-01T06:07:18.692959Z","shell.execute_reply":"2022-07-01T06:07:20.543497Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"CFG class now includes a new parameter: `decoder_layers`. For illustration purposes, I am running a two-layer LSTM for 1 epoch on 100k images.","metadata":{}},{"cell_type":"code","source":"print(timm.list_models(pretrained=True))","metadata":{"execution":{"iopub.status.busy":"2022-07-01T06:07:20.549142Z","iopub.execute_input":"2022-07-01T06:07:20.549441Z","iopub.status.idle":"2022-07-01T06:07:20.559442Z","shell.execute_reply.started":"2022-07-01T06:07:20.54941Z","shell.execute_reply":"2022-07-01T06:07:20.558593Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"['adv_inception_v3', 'bat_resnext26ts', 'beit_base_patch16_224', 'beit_base_patch16_224_in22k', 'beit_base_patch16_384', 'beit_large_patch16_224', 'beit_large_patch16_224_in22k', 'beit_large_patch16_384', 'beit_large_patch16_512', 'botnet26t_256', 'cait_m36_384', 'cait_m48_448', 'cait_s24_224', 'cait_s24_384', 'cait_s36_384', 'cait_xs24_384', 'cait_xxs24_224', 'cait_xxs24_384', 'cait_xxs36_224', 'cait_xxs36_384', 'coat_lite_mini', 'coat_lite_small', 'coat_lite_tiny', 'coat_mini', 'coat_tiny', 'convit_base', 'convit_small', 'convit_tiny', 'convmixer_768_32', 'convmixer_1024_20_ks9_p14', 'convmixer_1536_20', 'convnext_base', 'convnext_base_384_in22ft1k', 'convnext_base_in22ft1k', 'convnext_base_in22k', 'convnext_large', 'convnext_large_384_in22ft1k', 'convnext_large_in22ft1k', 'convnext_large_in22k', 'convnext_small', 'convnext_tiny', 'convnext_xlarge_384_in22ft1k', 'convnext_xlarge_in22ft1k', 'convnext_xlarge_in22k', 'crossvit_9_240', 'crossvit_9_dagger_240', 'crossvit_15_240', 'crossvit_15_dagger_240', 'crossvit_15_dagger_408', 'crossvit_18_240', 'crossvit_18_dagger_240', 'crossvit_18_dagger_408', 'crossvit_base_240', 'crossvit_small_240', 'crossvit_tiny_240', 'cspdarknet53', 'cspresnet50', 'cspresnext50', 'deit_base_distilled_patch16_224', 'deit_base_distilled_patch16_384', 'deit_base_patch16_224', 'deit_base_patch16_384', 'deit_small_distilled_patch16_224', 'deit_small_patch16_224', 'deit_tiny_distilled_patch16_224', 'deit_tiny_patch16_224', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'densenetblur121d', 'dla34', 'dla46_c', 'dla46x_c', 'dla60', 'dla60_res2net', 'dla60_res2next', 'dla60x', 'dla60x_c', 'dla102', 'dla102x', 'dla102x2', 'dla169', 'dm_nfnet_f0', 'dm_nfnet_f1', 'dm_nfnet_f2', 'dm_nfnet_f3', 'dm_nfnet_f4', 'dm_nfnet_f5', 'dm_nfnet_f6', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn107', 'dpn131', 'eca_botnext26ts_256', 'eca_halonext26ts', 'eca_nfnet_l0', 'eca_nfnet_l1', 'eca_nfnet_l2', 'eca_resnet33ts', 'eca_resnext26ts', 'ecaresnet26t', 'ecaresnet50d', 'ecaresnet50d_pruned', 'ecaresnet50t', 'ecaresnet101d', 'ecaresnet101d_pruned', 'ecaresnet269d', 'ecaresnetlight', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b1_pruned', 'efficientnet_b2', 'efficientnet_b2_pruned', 'efficientnet_b3', 'efficientnet_b3_pruned', 'efficientnet_b4', 'efficientnet_el', 'efficientnet_el_pruned', 'efficientnet_em', 'efficientnet_es', 'efficientnet_es_pruned', 'efficientnet_lite0', 'efficientnetv2_rw_m', 'efficientnetv2_rw_s', 'efficientnetv2_rw_t', 'ens_adv_inception_resnet_v2', 'ese_vovnet19b_dw', 'ese_vovnet39b', 'fbnetc_100', 'fbnetv3_b', 'fbnetv3_d', 'fbnetv3_g', 'gc_efficientnetv2_rw_t', 'gcresnet33ts', 'gcresnet50t', 'gcresnext26ts', 'gcresnext50ts', 'gernet_l', 'gernet_m', 'gernet_s', 'ghostnet_100', 'gluon_inception_v3', 'gluon_resnet18_v1b', 'gluon_resnet34_v1b', 'gluon_resnet50_v1b', 'gluon_resnet50_v1c', 'gluon_resnet50_v1d', 'gluon_resnet50_v1s', 'gluon_resnet101_v1b', 'gluon_resnet101_v1c', 'gluon_resnet101_v1d', 'gluon_resnet101_v1s', 'gluon_resnet152_v1b', 'gluon_resnet152_v1c', 'gluon_resnet152_v1d', 'gluon_resnet152_v1s', 'gluon_resnext50_32x4d', 'gluon_resnext101_32x4d', 'gluon_resnext101_64x4d', 'gluon_senet154', 'gluon_seresnext50_32x4d', 'gluon_seresnext101_32x4d', 'gluon_seresnext101_64x4d', 'gluon_xception65', 'gmixer_24_224', 'gmlp_s16_224', 'halo2botnet50ts_256', 'halonet26t', 'halonet50ts', 'haloregnetz_b', 'hardcorenas_a', 'hardcorenas_b', 'hardcorenas_c', 'hardcorenas_d', 'hardcorenas_e', 'hardcorenas_f', 'hrnet_w18', 'hrnet_w18_small', 'hrnet_w18_small_v2', 'hrnet_w30', 'hrnet_w32', 'hrnet_w40', 'hrnet_w44', 'hrnet_w48', 'hrnet_w64', 'ig_resnext101_32x8d', 'ig_resnext101_32x16d', 'ig_resnext101_32x32d', 'ig_resnext101_32x48d', 'inception_resnet_v2', 'inception_v3', 'inception_v4', 'jx_nest_base', 'jx_nest_small', 'jx_nest_tiny', 'lambda_resnet26rpt_256', 'lambda_resnet26t', 'lambda_resnet50ts', 'lamhalobotnet50ts_256', 'lcnet_050', 'lcnet_075', 'lcnet_100', 'legacy_senet154', 'legacy_seresnet18', 'legacy_seresnet34', 'legacy_seresnet50', 'legacy_seresnet101', 'legacy_seresnet152', 'legacy_seresnext26_32x4d', 'legacy_seresnext50_32x4d', 'legacy_seresnext101_32x4d', 'levit_128', 'levit_128s', 'levit_192', 'levit_256', 'levit_384', 'mixer_b16_224', 'mixer_b16_224_in21k', 'mixer_b16_224_miil', 'mixer_b16_224_miil_in21k', 'mixer_l16_224', 'mixer_l16_224_in21k', 'mixnet_l', 'mixnet_m', 'mixnet_s', 'mixnet_xl', 'mnasnet_100', 'mnasnet_small', 'mobilenetv2_050', 'mobilenetv2_100', 'mobilenetv2_110d', 'mobilenetv2_120d', 'mobilenetv2_140', 'mobilenetv3_large_100', 'mobilenetv3_large_100_miil', 'mobilenetv3_large_100_miil_in21k', 'mobilenetv3_rw', 'nasnetalarge', 'nf_regnet_b1', 'nf_resnet50', 'nfnet_l0', 'pit_b_224', 'pit_b_distilled_224', 'pit_s_224', 'pit_s_distilled_224', 'pit_ti_224', 'pit_ti_distilled_224', 'pit_xs_224', 'pit_xs_distilled_224', 'pnasnet5large', 'regnetx_002', 'regnetx_004', 'regnetx_006', 'regnetx_008', 'regnetx_016', 'regnetx_032', 'regnetx_040', 'regnetx_064', 'regnetx_080', 'regnetx_120', 'regnetx_160', 'regnetx_320', 'regnety_002', 'regnety_004', 'regnety_006', 'regnety_008', 'regnety_016', 'regnety_032', 'regnety_040', 'regnety_064', 'regnety_080', 'regnety_120', 'regnety_160', 'regnety_320', 'regnetz_b16', 'regnetz_c16', 'regnetz_d8', 'regnetz_d32', 'regnetz_e8', 'repvgg_a2', 'repvgg_b0', 'repvgg_b1', 'repvgg_b1g4', 'repvgg_b2', 'repvgg_b2g4', 'repvgg_b3', 'repvgg_b3g4', 'res2net50_14w_8s', 'res2net50_26w_4s', 'res2net50_26w_6s', 'res2net50_26w_8s', 'res2net50_48w_2s', 'res2net101_26w_4s', 'res2next50', 'resmlp_12_224', 'resmlp_12_224_dino', 'resmlp_12_distilled_224', 'resmlp_24_224', 'resmlp_24_224_dino', 'resmlp_24_distilled_224', 'resmlp_36_224', 'resmlp_36_distilled_224', 'resmlp_big_24_224', 'resmlp_big_24_224_in22ft1k', 'resmlp_big_24_distilled_224', 'resnest14d', 'resnest26d', 'resnest50d', 'resnest50d_1s4x24d', 'resnest50d_4s2x40d', 'resnest101e', 'resnest200e', 'resnest269e', 'resnet18', 'resnet18d', 'resnet26', 'resnet26d', 'resnet26t', 'resnet32ts', 'resnet33ts', 'resnet34', 'resnet34d', 'resnet50', 'resnet50_gn', 'resnet50d', 'resnet51q', 'resnet61q', 'resnet101', 'resnet101d', 'resnet152', 'resnet152d', 'resnet200d', 'resnetblur50', 'resnetrs50', 'resnetrs101', 'resnetrs152', 'resnetrs200', 'resnetrs270', 'resnetrs350', 'resnetrs420', 'resnetv2_50', 'resnetv2_50x1_bit_distilled', 'resnetv2_50x1_bitm', 'resnetv2_50x1_bitm_in21k', 'resnetv2_50x3_bitm', 'resnetv2_50x3_bitm_in21k', 'resnetv2_101', 'resnetv2_101x1_bitm', 'resnetv2_101x1_bitm_in21k', 'resnetv2_101x3_bitm', 'resnetv2_101x3_bitm_in21k', 'resnetv2_152x2_bit_teacher', 'resnetv2_152x2_bit_teacher_384', 'resnetv2_152x2_bitm', 'resnetv2_152x2_bitm_in21k', 'resnetv2_152x4_bitm', 'resnetv2_152x4_bitm_in21k', 'resnext26ts', 'resnext50_32x4d', 'resnext50d_32x4d', 'resnext101_32x8d', 'rexnet_100', 'rexnet_130', 'rexnet_150', 'rexnet_200', 'sebotnet33ts_256', 'sehalonet33ts', 'selecsls42b', 'selecsls60', 'selecsls60b', 'semnasnet_075', 'semnasnet_100', 'seresnet33ts', 'seresnet50', 'seresnet152d', 'seresnext26d_32x4d', 'seresnext26t_32x4d', 'seresnext26ts', 'seresnext50_32x4d', 'skresnet18', 'skresnet34', 'skresnext50_32x4d', 'spnasnet_100', 'ssl_resnet18', 'ssl_resnet50', 'ssl_resnext50_32x4d', 'ssl_resnext101_32x4d', 'ssl_resnext101_32x8d', 'ssl_resnext101_32x16d', 'swin_base_patch4_window7_224', 'swin_base_patch4_window7_224_in22k', 'swin_base_patch4_window12_384', 'swin_base_patch4_window12_384_in22k', 'swin_large_patch4_window7_224', 'swin_large_patch4_window7_224_in22k', 'swin_large_patch4_window12_384', 'swin_large_patch4_window12_384_in22k', 'swin_small_patch4_window7_224', 'swin_tiny_patch4_window7_224', 'swsl_resnet18', 'swsl_resnet50', 'swsl_resnext50_32x4d', 'swsl_resnext101_32x4d', 'swsl_resnext101_32x8d', 'swsl_resnext101_32x16d', 'tf_efficientnet_b0', 'tf_efficientnet_b0_ap', 'tf_efficientnet_b0_ns', 'tf_efficientnet_b1', 'tf_efficientnet_b1_ap', 'tf_efficientnet_b1_ns', 'tf_efficientnet_b2', 'tf_efficientnet_b2_ap', 'tf_efficientnet_b2_ns', 'tf_efficientnet_b3', 'tf_efficientnet_b3_ap', 'tf_efficientnet_b3_ns', 'tf_efficientnet_b4', 'tf_efficientnet_b4_ap', 'tf_efficientnet_b4_ns', 'tf_efficientnet_b5', 'tf_efficientnet_b5_ap', 'tf_efficientnet_b5_ns', 'tf_efficientnet_b6', 'tf_efficientnet_b6_ap', 'tf_efficientnet_b6_ns', 'tf_efficientnet_b7', 'tf_efficientnet_b7_ap', 'tf_efficientnet_b7_ns', 'tf_efficientnet_b8', 'tf_efficientnet_b8_ap', 'tf_efficientnet_cc_b0_4e', 'tf_efficientnet_cc_b0_8e', 'tf_efficientnet_cc_b1_8e', 'tf_efficientnet_el', 'tf_efficientnet_em', 'tf_efficientnet_es', 'tf_efficientnet_l2_ns', 'tf_efficientnet_l2_ns_475', 'tf_efficientnet_lite0', 'tf_efficientnet_lite1', 'tf_efficientnet_lite2', 'tf_efficientnet_lite3', 'tf_efficientnet_lite4', 'tf_efficientnetv2_b0', 'tf_efficientnetv2_b1', 'tf_efficientnetv2_b2', 'tf_efficientnetv2_b3', 'tf_efficientnetv2_l', 'tf_efficientnetv2_l_in21ft1k', 'tf_efficientnetv2_l_in21k', 'tf_efficientnetv2_m', 'tf_efficientnetv2_m_in21ft1k', 'tf_efficientnetv2_m_in21k', 'tf_efficientnetv2_s', 'tf_efficientnetv2_s_in21ft1k', 'tf_efficientnetv2_s_in21k', 'tf_efficientnetv2_xl_in21ft1k', 'tf_efficientnetv2_xl_in21k', 'tf_inception_v3', 'tf_mixnet_l', 'tf_mixnet_m', 'tf_mixnet_s', 'tf_mobilenetv3_large_075', 'tf_mobilenetv3_large_100', 'tf_mobilenetv3_large_minimal_100', 'tf_mobilenetv3_small_075', 'tf_mobilenetv3_small_100', 'tf_mobilenetv3_small_minimal_100', 'tinynet_a', 'tinynet_b', 'tinynet_c', 'tinynet_d', 'tinynet_e', 'tnt_s_patch16_224', 'tresnet_l', 'tresnet_l_448', 'tresnet_m', 'tresnet_m_448', 'tresnet_m_miil_in21k', 'tresnet_xl', 'tresnet_xl_448', 'tv_densenet121', 'tv_resnet34', 'tv_resnet50', 'tv_resnet101', 'tv_resnet152', 'tv_resnext50_32x4d', 'twins_pcpvt_base', 'twins_pcpvt_large', 'twins_pcpvt_small', 'twins_svt_base', 'twins_svt_large', 'twins_svt_small', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'visformer_small', 'vit_base_patch8_224', 'vit_base_patch8_224_dino', 'vit_base_patch8_224_in21k', 'vit_base_patch16_224', 'vit_base_patch16_224_dino', 'vit_base_patch16_224_in21k', 'vit_base_patch16_224_miil', 'vit_base_patch16_224_miil_in21k', 'vit_base_patch16_224_sam', 'vit_base_patch16_384', 'vit_base_patch32_224', 'vit_base_patch32_224_in21k', 'vit_base_patch32_224_sam', 'vit_base_patch32_384', 'vit_base_r50_s16_224_in21k', 'vit_base_r50_s16_384', 'vit_huge_patch14_224_in21k', 'vit_large_patch16_224', 'vit_large_patch16_224_in21k', 'vit_large_patch16_384', 'vit_large_patch32_224_in21k', 'vit_large_patch32_384', 'vit_large_r50_s32_224', 'vit_large_r50_s32_224_in21k', 'vit_large_r50_s32_384', 'vit_small_patch8_224_dino', 'vit_small_patch16_224', 'vit_small_patch16_224_dino', 'vit_small_patch16_224_in21k', 'vit_small_patch16_384', 'vit_small_patch32_224', 'vit_small_patch32_224_in21k', 'vit_small_patch32_384', 'vit_small_r26_s32_224', 'vit_small_r26_s32_224_in21k', 'vit_small_r26_s32_384', 'vit_tiny_patch16_224', 'vit_tiny_patch16_224_in21k', 'vit_tiny_patch16_384', 'vit_tiny_r_s16_p8_224', 'vit_tiny_r_s16_p8_224_in21k', 'vit_tiny_r_s16_p8_384', 'wide_resnet50_2', 'wide_resnet101_2', 'xception', 'xception41', 'xception65', 'xception71', 'xcit_large_24_p8_224', 'xcit_large_24_p8_224_dist', 'xcit_large_24_p8_384_dist', 'xcit_large_24_p16_224', 'xcit_large_24_p16_224_dist', 'xcit_large_24_p16_384_dist', 'xcit_medium_24_p8_224', 'xcit_medium_24_p8_224_dist', 'xcit_medium_24_p8_384_dist', 'xcit_medium_24_p16_224', 'xcit_medium_24_p16_224_dist', 'xcit_medium_24_p16_384_dist', 'xcit_nano_12_p8_224', 'xcit_nano_12_p8_224_dist', 'xcit_nano_12_p8_384_dist', 'xcit_nano_12_p16_224', 'xcit_nano_12_p16_224_dist', 'xcit_nano_12_p16_384_dist', 'xcit_small_12_p8_224', 'xcit_small_12_p8_224_dist', 'xcit_small_12_p8_384_dist', 'xcit_small_12_p16_224', 'xcit_small_12_p16_224_dist', 'xcit_small_12_p16_384_dist', 'xcit_small_24_p8_224', 'xcit_small_24_p8_224_dist', 'xcit_small_24_p8_384_dist', 'xcit_small_24_p16_224', 'xcit_small_24_p16_224_dist', 'xcit_small_24_p16_384_dist', 'xcit_tiny_12_p8_224', 'xcit_tiny_12_p8_224_dist', 'xcit_tiny_12_p8_384_dist', 'xcit_tiny_12_p16_224', 'xcit_tiny_12_p16_224_dist', 'xcit_tiny_12_p16_384_dist', 'xcit_tiny_24_p8_224', 'xcit_tiny_24_p8_224_dist', 'xcit_tiny_24_p8_384_dist', 'xcit_tiny_24_p16_224', 'xcit_tiny_24_p16_224_dist', 'xcit_tiny_24_p16_384_dist']\n","output_type":"stream"}]},{"cell_type":"code","source":"#  n_channels_dict = {'efficientnet-b0': 1280, 'efficientnet-b1': 1280, 'efficientnet-b2': 1408,\n#   'efficientnet-b3': 1536, 'efficientnet-b4': 1792, 'efficientnet-b5': 2048,\n#   'efficientnet-b6': 2304, 'efficientnet-b7': 2560}\n\n# This is not, to put it mildly, the most elegant solution ever - but I ran into some trouble \n# with checking the size of feature spaces programmatically inside the CFG definition.\n\nclass CFG:\n    debug          = True\n    apex           = False\n    max_len        = 275\n    print_freq     = 250\n    num_workers    = 4\n    model_name     = 'efficientnet_b2'\n    enc_size       = 1408\n#     model_name     = 'mobilenetv2_100'\n#     enc_size       = 1280\n#     model_name     = 'tnt_s_patch16_224'\n#     enc_size       = 384\n#     model_name     = 'vit_base_patch16_224'\n#     enc_size       = 768\n#     model_name     = 'resnet50'\n#     enc_size       = 2048\n    samp_size      = 100000\n    size           = 288\n#     size           = 224\n    scheduler      = 'CosineAnnealingLR' \n    epochs         = 10\n    T_max          = 4  \n    encoder_lr     = 1e-4\n    decoder_lr     = 4e-4\n    min_lr         = 1e-6\n    batch_size     = 32\n    weight_decay   = 1e-6\n    gradient_accumulation_steps = 1\n    max_grad_norm  = 10\n    attention_dim  = 256\n    embed_dim      = 512\n    decoder_dim    = 512\n    decoder_layers = 2     # number of LSTM layers\n    dropout        = 0.5\n    seed           = 42\n    n_fold         = 5\n    trn_fold       = 0 \n    train          = True\n    train_path     = '../input/bms-molecular-translation/'\n    prep_path      = '../input/preprocessed-stuff/'\n    prev_model     = './prp/muh_best.pth'","metadata":{"papermill":{"duration":0.022485,"end_time":"2021-04-07T08:27:10.713561","exception":false,"start_time":"2021-04-07T08:27:10.691076","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:20.560887Z","iopub.execute_input":"2022-07-01T06:07:20.561657Z","iopub.status.idle":"2022-07-01T06:07:20.569929Z","shell.execute_reply.started":"2022-07-01T06:07:20.561619Z","shell.execute_reply":"2022-07-01T06:07:20.568989Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{"papermill":{"duration":0.011683,"end_time":"2021-04-07T08:27:10.737552","exception":false,"start_time":"2021-04-07T08:27:10.725869","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Tokenizer(object):\n    \n    def __init__(self):\n        self.stoi = {}\n        self.itos = {}\n\n    def __len__(self):\n        return len(self.stoi)\n    \n    def fit_on_texts(self, texts):\n        vocab = set()\n        for text in texts:\n            vocab.update(text.split(' '))\n        vocab = sorted(vocab)\n        vocab.append('<sos>')\n        vocab.append('<eos>')\n        vocab.append('<pad>')\n        for i, s in enumerate(vocab):\n            self.stoi[s] = i\n        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n        \n    def text_to_sequence(self, text):\n        sequence = []\n        sequence.append(self.stoi['<sos>'])\n        for s in text.split(' '):\n            sequence.append(self.stoi[s])\n        sequence.append(self.stoi['<eos>'])\n        return sequence\n    \n    def texts_to_sequences(self, texts):\n        sequences = []\n        for text in texts:\n            sequence = self.text_to_sequence(text)\n            sequences.append(sequence)\n        return sequences\n\n    def sequence_to_text(self, sequence):\n        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n    \n    def sequences_to_texts(self, sequences):\n        texts = []\n        for sequence in sequences:\n            text = self.sequence_to_text(sequence)\n            texts.append(text)\n        return texts\n    \n    def predict_caption(self, sequence):\n        caption = ''\n        for i in sequence:\n            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n                break\n            caption += self.itos[i]\n        return caption\n    \n    def predict_captions(self, sequences):\n        captions = []\n        for sequence in sequences:\n            caption = self.predict_caption(sequence)\n            captions.append(caption)\n        return captions\n\ntokenizer = torch.load(CFG.prep_path + 'tokenizer2.pth')\nprint(f\"tokenizer.stoi: {tokenizer.stoi}\")","metadata":{"papermill":{"duration":0.037989,"end_time":"2021-04-07T08:27:10.787242","exception":false,"start_time":"2021-04-07T08:27:10.749253","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:20.572408Z","iopub.execute_input":"2022-07-01T06:07:20.573258Z","iopub.status.idle":"2022-07-01T06:07:20.59073Z","shell.execute_reply.started":"2022-07-01T06:07:20.573233Z","shell.execute_reply":"2022-07-01T06:07:20.589875Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tokenizer.stoi: {'(': 0, ')': 1, '+': 2, ',': 3, '-': 4, '/b': 5, '/c': 6, '/h': 7, '/i': 8, '/m': 9, '/s': 10, '/t': 11, '0': 12, '1': 13, '10': 14, '100': 15, '101': 16, '102': 17, '103': 18, '104': 19, '105': 20, '106': 21, '107': 22, '108': 23, '109': 24, '11': 25, '110': 26, '111': 27, '112': 28, '113': 29, '114': 30, '115': 31, '116': 32, '117': 33, '118': 34, '119': 35, '12': 36, '120': 37, '121': 38, '122': 39, '123': 40, '124': 41, '125': 42, '126': 43, '127': 44, '128': 45, '129': 46, '13': 47, '130': 48, '131': 49, '132': 50, '133': 51, '134': 52, '135': 53, '136': 54, '137': 55, '138': 56, '139': 57, '14': 58, '140': 59, '141': 60, '142': 61, '143': 62, '144': 63, '145': 64, '146': 65, '147': 66, '148': 67, '149': 68, '15': 69, '150': 70, '151': 71, '152': 72, '153': 73, '154': 74, '155': 75, '156': 76, '157': 77, '158': 78, '159': 79, '16': 80, '161': 81, '163': 82, '165': 83, '167': 84, '17': 85, '18': 86, '19': 87, '2': 88, '20': 89, '21': 90, '22': 91, '23': 92, '24': 93, '25': 94, '26': 95, '27': 96, '28': 97, '29': 98, '3': 99, '30': 100, '31': 101, '32': 102, '33': 103, '34': 104, '35': 105, '36': 106, '37': 107, '38': 108, '39': 109, '4': 110, '40': 111, '41': 112, '42': 113, '43': 114, '44': 115, '45': 116, '46': 117, '47': 118, '48': 119, '49': 120, '5': 121, '50': 122, '51': 123, '52': 124, '53': 125, '54': 126, '55': 127, '56': 128, '57': 129, '58': 130, '59': 131, '6': 132, '60': 133, '61': 134, '62': 135, '63': 136, '64': 137, '65': 138, '66': 139, '67': 140, '68': 141, '69': 142, '7': 143, '70': 144, '71': 145, '72': 146, '73': 147, '74': 148, '75': 149, '76': 150, '77': 151, '78': 152, '79': 153, '8': 154, '80': 155, '81': 156, '82': 157, '83': 158, '84': 159, '85': 160, '86': 161, '87': 162, '88': 163, '89': 164, '9': 165, '90': 166, '91': 167, '92': 168, '93': 169, '94': 170, '95': 171, '96': 172, '97': 173, '98': 174, '99': 175, 'B': 176, 'Br': 177, 'C': 178, 'Cl': 179, 'D': 180, 'F': 181, 'H': 182, 'I': 183, 'N': 184, 'O': 185, 'P': 186, 'S': 187, 'Si': 188, 'T': 189, '<sos>': 190, '<eos>': 191, '<pad>': 192}\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_score(y_true, y_pred):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        score = Levenshtein.distance(true, pred)\n        scores.append(score)\n    avg_score = np.mean(scores)\n    return avg_score\n\n\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\n\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed = CFG.seed)","metadata":{"papermill":{"duration":0.027729,"end_time":"2021-04-07T08:27:10.827442","exception":false,"start_time":"2021-04-07T08:27:10.799713","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:20.595145Z","iopub.execute_input":"2022-07-01T06:07:20.59541Z","iopub.status.idle":"2022-07-01T06:07:20.606785Z","shell.execute_reply.started":"2022-07-01T06:07:20.595386Z","shell.execute_reply":"2022-07-01T06:07:20.605856Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\n\nclass TrainDataset(Dataset):\n    def __init__(self, df, tokenizer, transform=None):\n        super().__init__()\n        self.df         = df\n        self.tokenizer  = tokenizer\n        self.file_paths = df['file_path'].values\n        self.labels     = df['InChI_text'].values\n        self.transform  = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image = image)\n            image     = augmented['image']\n        label = self.labels[idx]\n        label = self.tokenizer.text_to_sequence(label)\n        label_length = len(label)\n        label_length = torch.LongTensor([label_length])\n        return image, torch.LongTensor(label), label_length\n    \n\nclass TestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        super().__init__()\n        self.df = df\n        self.file_paths = df['file_path'].values\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        return image","metadata":{"papermill":{"duration":0.024936,"end_time":"2021-04-07T08:27:10.869493","exception":false,"start_time":"2021-04-07T08:27:10.844557","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:20.609753Z","iopub.execute_input":"2022-07-01T06:07:20.610173Z","iopub.status.idle":"2022-07-01T06:07:20.623536Z","shell.execute_reply.started":"2022-07-01T06:07:20.610139Z","shell.execute_reply":"2022-07-01T06:07:20.6228Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def bms_collate(batch):\n    imgs, labels, label_lengths = [], [], []\n    for data_point in batch:\n        imgs.append(data_point[0])\n        labels.append(data_point[1])\n        label_lengths.append(data_point[2])\n    labels = pad_sequence(labels, batch_first = True, padding_value = tokenizer.stoi[\"<pad>\"])\n    return torch.stack(imgs), labels, torch.stack(label_lengths).reshape(-1, 1)","metadata":{"papermill":{"duration":0.021152,"end_time":"2021-04-07T08:27:10.902922","exception":false,"start_time":"2021-04-07T08:27:10.88177","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:20.625196Z","iopub.execute_input":"2022-07-01T06:07:20.625885Z","iopub.status.idle":"2022-07-01T06:07:20.634724Z","shell.execute_reply.started":"2022-07-01T06:07:20.625847Z","shell.execute_reply":"2022-07-01T06:07:20.633768Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"####### CNN ENCODER\n\nclass Encoder(nn.Module):\n    def __init__(self, model_name = CFG.model_name, pretrained = False):\n        super().__init__()\n        self.cnn = timm.create_model(model_name, pretrained = pretrained)\n\n    def forward(self, x):\n        bs       = x.size(0)\n        features = self.cnn.forward_features(x)\n        \n        features = features.permute(0, 2, 3, 1)\n\n        return features\n    \ndecoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)","metadata":{"papermill":{"duration":0.021209,"end_time":"2021-04-07T08:27:10.936685","exception":false,"start_time":"2021-04-07T08:27:10.915476","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:20.637903Z","iopub.execute_input":"2022-07-01T06:07:20.638144Z","iopub.status.idle":"2022-07-01T06:07:20.645579Z","shell.execute_reply.started":"2022-07-01T06:07:20.638121Z","shell.execute_reply":"2022-07-01T06:07:20.644813Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The class `DecoderWithAttention` is updated to support a multi-layer LSTM.","metadata":{}},{"cell_type":"code","source":"####### RNN DECODER\n\n# attention module\nclass Attention(nn.Module):\n    '''\n    Attention network for calculate attention value\n    '''\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        '''\n        :param encoder_dim: input size of encoder network\n        :param decoder_dim: input size of decoder network\n        :param attention_dim: input size of attention network\n        '''\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n        self.full_att    = nn.Linear(attention_dim, 1)            # linear layer to calculate values to be softmax-ed\n        self.relu        = nn.ReLU()\n        self.softmax     = nn.Softmax(dim = 1)  # softmax layer to calculate weights\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1  = self.encoder_att(encoder_out)     # (batch_size, num_pixels, attention_dim)\n        att2  = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att   = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)                 # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim = 1)  # (batch_size, encoder_dim)\n        return attention_weighted_encoding, alpha\n    \n    \n# custom LSTM cell\ndef LSTMCell(input_size, hidden_size, **kwargs):\n    m = nn.LSTMCell(input_size, hidden_size, **kwargs)\n    for name, param in m.named_parameters():\n        if 'weight' in name or 'bias' in name:\n            param.data.uniform_(-0.1, 0.1)\n    return m\n\n\n# decoder\nclass DecoderWithAttention(nn.Module):\n    '''\n    Decoder network with attention network used for training\n    '''\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim, dropout, num_layers):\n        '''\n        :param attention_dim: input size of attention network\n        :param embed_dim: input size of embedding network\n        :param decoder_dim: input size of decoder network\n        :param vocab_size: total number of characters used in training\n        :param encoder_dim: input size of encoder network\n        :param num_layers: number of the LSTM layers\n        :param dropout: dropout rate\n        '''\n        super(DecoderWithAttention, self).__init__()\n        self.encoder_dim   = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim     = embed_dim\n        self.decoder_dim   = decoder_dim\n        self.vocab_size    = vocab_size\n        self.dropout       = dropout\n        self.num_layers    = num_layers\n        self.device        = device\n        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n        self.embedding     = nn.Embedding(vocab_size, embed_dim)                 # embedding layer\n        self.dropout       = nn.Dropout(p = self.dropout)\n        self.decode_step   = nn.ModuleList([LSTMCell(embed_dim + encoder_dim if layer == 0 else embed_dim, embed_dim) for layer in range(self.num_layers)]) # decoding LSTMCell        \n        self.init_h        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta        = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid       = nn.Sigmoid()\n        self.fc            = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()                                      # initialize some layers with the uniform distribution\n\n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune = True):\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim = 1)\n        h = [self.init_h(mean_encoder_out) for i in range(self.num_layers)]  # (batch_size, decoder_dim)\n        c = [self.init_c(mean_encoder_out) for i in range(self.num_layers)]\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        '''\n        :param encoder_out: output of encoder network\n        :param encoded_captions: transformed sequence from character to integer\n        :param caption_lengths: length of transformed sequence\n        '''\n        batch_size       = encoder_out.size(0)\n        encoder_dim      = encoder_out.size(-1)\n        vocab_size       = self.vocab_size\n        encoder_out      = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels       = encoder_out.size(1)\n        \n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim = 0, descending = True)\n        encoder_out      = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n        \n        \n        # embedding transformed sequence for vector\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n        \n        # Initialize LSTM state, initialize cell_vector and hidden_vector\n        prev_h, prev_c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        \n        # set decode length by caption length - 1 because of omitting start token\n        decode_lengths = (caption_lengths - 1).tolist()\n        predictions    = torch.zeros(batch_size, max(decode_lengths), vocab_size, device = self.device)\n        alphas         = torch.zeros(batch_size, max(decode_lengths), num_pixels, device = self.device)\n        \n        # predict sequence\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                prev_h[-1][:batch_size_t])\n            gate = self.sigmoid(self.f_beta(prev_h[-1][:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n\n            input = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n            \n            for i, rnn in enumerate(self.decode_step):\n                # recurrent cell\n                h, c = rnn(input, (prev_h[i][:batch_size_t], prev_c[i][:batch_size_t])) # cell_vector and hidden_vector\n\n                # hidden state becomes the input to the next layer\n                input = self.dropout(h)\n\n                # save state for next time step\n                prev_h[i] = h\n                prev_c[i] = c\n                \n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :]      = alpha\n            \n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n    \n    def predict(self, encoder_out, decode_lengths, tokenizer):\n        \n        # size variables\n        batch_size  = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size  = self.vocab_size\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels  = encoder_out.size(1)\n        \n        # embed start tocken for LSTM input\n        start_tockens = torch.ones(batch_size, dtype = torch.long, device = self.device) * tokenizer.stoi['<sos>']\n        embeddings    = self.embedding(start_tockens)\n        \n        # initialize hidden state and cell state of LSTM cell\n        h, c        = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        predictions = torch.zeros(batch_size, decode_lengths, vocab_size, device = self.device)\n        \n        # predict sequence\n        end_condition = torch.zeros(batch_size, dtype=torch.long, device = self.device)\n        for t in range(decode_lengths):\n            awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n            gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n            awe        = gate * awe\n            \n            input = torch.cat([embeddings, awe], dim=1)\n \n            for j, rnn in enumerate(self.decode_step):\n                at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n                input = self.dropout(at_h)\n                h[j]  = at_h\n                c[j]  = at_c\n            \n            preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n            predictions[:, t, :] = preds\n            end_condition |= (torch.argmax(preds, -1) == tokenizer.stoi[\"<eos>\"])\n            if end_condition.sum() == batch_size:\n                break\n            embeddings = self.embedding(torch.argmax(preds, -1))\n        \n        return predictions\n    \n    # beam search\n    def forward_step(self, prev_tokens, hidden, encoder_out, function):\n        \n        h, c = hidden\n        #h, c = h.squeeze(0), c.squeeze(0)\n        h, c = [hi.squeeze(0) for hi in h], [ci.squeeze(0) for ci in c]\n        \n        embeddings = self.embedding(prev_tokens)\n        if embeddings.dim() == 3:\n            embeddings = embeddings.squeeze(1)\n            \n        awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n        gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n        awe        = gate * awe\n        \n        input = torch.cat([embeddings, awe], dim = 1)\n        for j, rnn in enumerate(self.decode_step):\n            at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n            input = self.dropout(at_h)\n            h[j]  = at_h\n            c[j]  = at_c\n\n        preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n\n        #hidden = (h.unsqueeze(0), c.unsqueeze(0))\n        hidden = [hi.unsqueeze(0) for hi in h], [ci.unsqueeze(0) for ci in c]\n        predicted_softmax = function(preds, dim = 1)\n        \n        return predicted_softmax, hidden, None","metadata":{"papermill":{"duration":0.067717,"end_time":"2021-04-07T08:27:11.017304","exception":false,"start_time":"2021-04-07T08:27:10.949587","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:20.648143Z","iopub.execute_input":"2022-07-01T06:07:20.64893Z","iopub.status.idle":"2022-07-01T06:07:20.688369Z","shell.execute_reply.started":"2022-07-01T06:07:20.648892Z","shell.execute_reply":"2022-07-01T06:07:20.687676Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Helper functions\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val   = 0\n        self.avg   = 0\n        self.sum   = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val    = val\n        self.sum   += val * n\n        self.count += n\n        self.avg    = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s   = now - since\n    es  = s / (percent)\n    rs  = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\ndef train_fn(train_loader, encoder, decoder, criterion, \n             encoder_optimizer, decoder_optimizer, epoch,\n             encoder_scheduler, decoder_scheduler, device):\n    \n    batch_time = AverageMeter()\n    data_time  = AverageMeter()\n    losses     = AverageMeter()\n    \n    # switch to train mode\n    encoder.train()\n    decoder.train()\n    \n    start = end = time.time()\n    global_step = 0\n    \n#     text_preds = []\n    \n    for step, (images, labels, label_lengths) in enumerate(train_loader):\n        \n        # measure data loading time\n        data_time.update(time.time() - end)\n        \n        images        = images.to(device)\n        labels        = labels.to(device)\n        label_lengths = label_lengths.to(device)\n        batch_size    = images.size(0)\n        \n        \n        features = encoder(images)\n        predictions, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, labels, label_lengths)\n        targets     = caps_sorted[:, 1:]\n        predictions = pack_padded_sequence(predictions, decode_lengths, batch_first=True).data\n        targets     = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n        loss        = criterion(predictions, targets)\n\n        \n        # record loss\n        losses.update(loss.item(), batch_size)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n            \n        if CFG.apex:\n            with amp.scale_loss(loss, decoder_optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n            \n        encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), CFG.max_grad_norm)\n        decoder_grad_norm = torch.nn.utils.clip_grad_norm_(decoder.parameters(), CFG.max_grad_norm)\n        \n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            global_step += 1\n            \n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Encoder Grad: {encoder_grad_norm:.4f}  '\n                  'Decoder Grad: {decoder_grad_norm:.4f}  '\n                  #'Encoder LR: {encoder_lr:.6f}  '\n                  #'Decoder LR: {decoder_lr:.6f}  '\n                  .format(\n                   epoch+1, step, len(train_loader), \n                   batch_time        = batch_time,\n                   data_time         = data_time, \n                   loss              = losses,\n                   remain            = timeSince(start, float(step+1)/len(train_loader)),\n                   encoder_grad_norm = encoder_grad_norm,\n                   decoder_grad_norm = decoder_grad_norm,\n                   #encoder_lr=encoder_scheduler.get_lr()[0],\n                   #decoder_lr=decoder_scheduler.get_lr()[0],\n                   ))\n                \n    return losses.avg\n\n\ndef valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device):\n    \n    batch_time = AverageMeter()\n    data_time  = AverageMeter()\n    losses     = AverageMeter()\n    \n    # switch to evaluation mode\n    encoder.eval()\n    decoder.eval()\n    \n    text_preds = []\n    start = end = time.time()\n    \n    for step, (images, labels, label_lengths) in enumerate(valid_loader):\n        \n        # measure data loading time\n        data_time.update(time.time() - end)\n        \n        images     = images.to(device)\n        labels        = labels.to(device)\n        label_lengths = label_lengths.to(device)\n        batch_size = images.size(0)\n        \n        with torch.no_grad():\n            features    = encoder(images)\n            predictions = decoder.predict(features, CFG.max_len, tokenizer)\n            \n        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n        _text_preds        = tokenizer.predict_captions(predicted_sequence)\n        text_preds.append(_text_preds)\n        \n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  .format(\n                   step, len(valid_loader), \n                   batch_time = batch_time,\n                   data_time  = data_time,\n                   remain     = timeSince(start, float(step+1)/len(valid_loader)),\n                   ))\n            \n\n        decode_lengths = (label_lengths.squeeze(1) - 1).tolist()\n        targets = labels[:, 1:]\n        predictions = pack_padded_sequence(predictions, decode_lengths, batch_first=True, enforce_sorted=False).data\n        targets     = pack_padded_sequence(targets, decode_lengths, batch_first=True, enforce_sorted=False).data\n        loss        = criterion(predictions, targets)\n        losses.update(loss.item(), batch_size)\n    \n    text_preds = np.concatenate(text_preds)\n    \n    return losses.avg, text_preds","metadata":{"papermill":{"duration":0.039205,"end_time":"2021-04-07T08:27:11.070219","exception":false,"start_time":"2021-04-07T08:27:11.031014","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:20.69126Z","iopub.execute_input":"2022-07-01T06:07:20.691676Z","iopub.status.idle":"2022-07-01T06:07:20.720355Z","shell.execute_reply.started":"2022-07-01T06:07:20.69162Z","shell.execute_reply":"2022-07-01T06:07:20.719381Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Train loop\n# ====================================================\nencoder = Encoder(CFG.model_name, pretrained = True)\n\ndecoder = DecoderWithAttention(attention_dim = CFG.attention_dim, \n                               embed_dim     = CFG.embed_dim, \n                               encoder_dim   = CFG.enc_size,\n                               decoder_dim   = CFG.decoder_dim,\n                               num_layers    = CFG.decoder_layers,\n                               vocab_size    = len(tokenizer), \n                               dropout       = CFG.dropout, \n                               device        = device)\n\ndef train_loop(folds, fold, encoder, decoder):\n\n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n\n    train_folds  = folds.loc[trn_idx].reset_index(drop = True)\n    valid_folds  = folds.loc[val_idx].reset_index(drop = True)\n    valid_labels = valid_folds['InChI'].values\n\n    train_dataset = TrainDataset(train_folds, tokenizer, transform = get_transforms(data = 'train'))\n#     valid_dataset = TestDataset(valid_folds, transform = get_transforms(data = 'valid'))\n    valid_dataset = TrainDataset(valid_folds, tokenizer, transform = get_transforms(data = 'valid'))\n\n\n    \n    train_loader = DataLoader(train_dataset, \n                              batch_size  = CFG.batch_size, \n                              shuffle     = True, \n                              num_workers = CFG.num_workers, \n                              pin_memory  = True,\n                              drop_last   = True, \n                              collate_fn  = bms_collate)\n#     valid_loader = DataLoader(valid_dataset, \n#                               batch_size  = CFG.batch_size, \n#                               shuffle     = False, \n#                               num_workers = CFG.num_workers,\n#                               pin_memory  = True, \n#                               drop_last   = False)\n    \n    valid_loader = DataLoader(valid_dataset, \n                              batch_size  = CFG.batch_size, \n                              shuffle     = False, \n                              num_workers = CFG.num_workers, \n                              pin_memory  = True,\n                              drop_last   = False, \n                              collate_fn  = bms_collate)\n    \n    # ====================================================\n    # scheduler \n    # ====================================================\n    def get_scheduler(optimizer):\n        if CFG.scheduler=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, \n                                          mode     = 'min', \n                                          factor   = CFG.factor, \n                                          patience = CFG.patience, \n                                          verbose  = True, \n                                          eps      = CFG.eps)\n        elif CFG.scheduler=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, \n                                          T_max      = CFG.T_max, \n                                          eta_min    = CFG.min_lr, \n                                          last_epoch = -1)\n        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, \n                                                    T_0        = CFG.T_0, \n                                                    T_mult     = 1, \n                                                    eta_min    = CFG.min_lr, \n                                                    last_epoch = -1)\n        return scheduler\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n\n#    states = torch.load(CFG.prev_model,  map_location=torch.device('cpu'))\n\n#    encoder = Encoder(CFG.model_name, pretrained = True)\n#    encoder.load_state_dict(states['encoder'])\n    \n    encoder.to(device)\n    encoder_optimizer = Adam(encoder.parameters(), \n                             lr           = CFG.encoder_lr, \n                             weight_decay = CFG.weight_decay, \n                             amsgrad      = False)\n#    encoder_optimizer.load_state_dict(states['encoder_optimizer'])\n    encoder_scheduler = get_scheduler(encoder_optimizer)\n#    encoder_scheduler.load_state_dict(states['encoder_scheduler'])\n    \n    \n#    decoder.load_state_dict(states['decoder'])\n    decoder.to(device)\n    decoder_optimizer = Adam(decoder.parameters(), \n                             lr           = CFG.decoder_lr, \n                             weight_decay = CFG.weight_decay, \n                             amsgrad      = False)\n#    decoder_optimizer.load_state_dict(states['decoder_optimizer'])\n\n    decoder_scheduler = get_scheduler(decoder_optimizer)\n #   decoder_scheduler.load_state_dict(states['decoder_scheduler'])\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.stoi[\"<pad>\"])\n\n    best_score = np.inf\n    best_loss  = np.inf\n    record_score = []\n    record_loss = []\n    record_loss_val = []\n    \n    for epoch in range(CFG.epochs):\n        \n        start_time = time.time()\n        \n        # train\n        avg_loss = train_fn(train_loader, encoder, decoder, criterion, \n                            encoder_optimizer, decoder_optimizer, epoch, \n                            encoder_scheduler, decoder_scheduler, device)\n\n        # eval\n        avg_loss_val, text_preds = valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device)\n        text_preds = [f\"InChI=1S/{text}\" for text in text_preds]\n        LOGGER.info(f\"val labels: {valid_labels[:5]}\")\n        LOGGER.info(f\"val preds: {text_preds[:5]}\")\n        \n        # scoring\n        score = get_score(valid_labels, text_preds)        \n        \n        if isinstance(encoder_scheduler, ReduceLROnPlateau):\n            encoder_scheduler.step(score)\n        elif isinstance(encoder_scheduler, CosineAnnealingLR):\n            encoder_scheduler.step()\n        elif isinstance(encoder_scheduler, CosineAnnealingWarmRestarts):\n            encoder_scheduler.step()\n            \n        if isinstance(decoder_scheduler, ReduceLROnPlateau):\n            decoder_scheduler.step(score)\n        elif isinstance(decoder_scheduler, CosineAnnealingLR):\n            decoder_scheduler.step()\n        elif isinstance(decoder_scheduler, CosineAnnealingWarmRestarts):\n            decoder_scheduler.step()\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - avg_val_loss: {avg_loss_val:.4f}')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n        \n        record_score.append(round(score, 4))\n        record_loss.append(round(avg_loss, 4))\n        record_loss_val.append(round(avg_loss_val, 4))\n        \n        if score < best_score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'encoder': encoder.state_dict(), \n                        'encoder_optimizer': encoder_optimizer.state_dict(), \n                        'encoder_scheduler': encoder_scheduler.state_dict(), \n                        'decoder': decoder.state_dict(), \n                        'decoder_optimizer': decoder_optimizer.state_dict(), \n                        'decoder_scheduler': decoder_scheduler.state_dict(), \n                        'text_preds': text_preds,\n                       },\n                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')\n          \n    print(\"validation scores\", record_score)\n    print(\"train losses\", record_loss)\n    print(\"Val loss\", record_loss_val)","metadata":{"papermill":{"duration":0.038367,"end_time":"2021-04-07T08:27:11.123364","exception":false,"start_time":"2021-04-07T08:27:11.084997","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:20.721972Z","iopub.execute_input":"2022-07-01T06:07:20.722582Z","iopub.status.idle":"2022-07-01T06:07:21.11975Z","shell.execute_reply.started":"2022-07-01T06:07:20.722544Z","shell.execute_reply":"2022-07-01T06:07:21.118796Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def get_train_file_path(image_id):\n\n    return CFG.train_path + \"train/{}/{}/{}/{}.png\".format(\n        image_id[0], image_id[1], image_id[2], image_id \n    )\n\ndef get_test_file_path(image_id):\n\n    return CFG.train_path + \"test/{}/{}/{}/{}.png\".format(\n        image_id[0], image_id[1], image_id[2], image_id \n    )","metadata":{"papermill":{"duration":0.020522,"end_time":"2021-04-07T08:27:11.156788","exception":false,"start_time":"2021-04-07T08:27:11.136266","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:21.121132Z","iopub.execute_input":"2022-07-01T06:07:21.121501Z","iopub.status.idle":"2022-07-01T06:07:21.127961Z","shell.execute_reply.started":"2022-07-01T06:07:21.121464Z","shell.execute_reply":"2022-07-01T06:07:21.127045Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# transformations\n\ndef get_transforms(*, data):\n    \n    if data == 'train':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            HorizontalFlip(p=0.5),                  \n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),   \n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n    \n    elif data == 'valid':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n","metadata":{"papermill":{"duration":0.022257,"end_time":"2021-04-07T08:27:11.192091","exception":false,"start_time":"2021-04-07T08:27:11.169834","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:21.129535Z","iopub.execute_input":"2022-07-01T06:07:21.130063Z","iopub.status.idle":"2022-07-01T06:07:21.138941Z","shell.execute_reply.started":"2022-07-01T06:07:21.130027Z","shell.execute_reply":"2022-07-01T06:07:21.138069Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{"papermill":{"duration":0.013404,"end_time":"2021-04-07T08:27:11.218463","exception":false,"start_time":"2021-04-07T08:27:11.205059","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train = pd.read_pickle(CFG.prep_path + 'train2.pkl')\n\ntrain['file_path'] = train['image_id'].apply(get_train_file_path)\n\nprint(f'train.shape: {train.shape}')\n\ntest = pd.read_csv('../input/bms-molecular-translation/sample_submission.csv')\n\ntest['file_path'] = test['image_id'].apply(get_test_file_path)\n\nprint(f'test.shape: {test.shape}')\n\n\nif CFG.debug:\n    # CFG.epochs = 1\n    train = train.sample(n = CFG.samp_size, random_state = CFG.seed).reset_index(drop = True)","metadata":{"papermill":{"duration":12.663809,"end_time":"2021-04-07T08:27:23.895404","exception":false,"start_time":"2021-04-07T08:27:11.231595","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:21.140639Z","iopub.execute_input":"2022-07-01T06:07:21.14116Z","iopub.status.idle":"2022-07-01T06:07:29.87342Z","shell.execute_reply.started":"2022-07-01T06:07:21.14112Z","shell.execute_reply":"2022-07-01T06:07:29.872466Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"train.shape: (2424186, 6)\ntest.shape: (1616107, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = TrainDataset(train, tokenizer, transform = get_transforms(data='train'))\n\nfolds = train.copy()\nFold = StratifiedKFold(n_splits = CFG.n_fold, shuffle = True, random_state = CFG.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds['InChI_length'])):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype(int)","metadata":{"papermill":{"duration":0.02079,"end_time":"2021-04-07T08:27:23.931533","exception":false,"start_time":"2021-04-07T08:27:23.910743","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:29.874872Z","iopub.execute_input":"2022-07-01T06:07:29.875217Z","iopub.status.idle":"2022-07-01T06:07:29.952193Z","shell.execute_reply.started":"2022-07-01T06:07:29.875181Z","shell.execute_reply":"2022-07-01T06:07:29.951296Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"papermill":{"duration":0.013921,"end_time":"2021-04-07T08:27:24.004031","exception":false,"start_time":"2021-04-07T08:27:23.99011","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_loop(folds, CFG.trn_fold, encoder, decoder)","metadata":{"papermill":{"duration":39.171513,"end_time":"2021-04-07T08:28:03.18932","exception":false,"start_time":"2021-04-07T08:27:24.017807","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-01T06:07:29.953384Z","iopub.execute_input":"2022-07-01T06:07:29.953745Z","iopub.status.idle":"2022-07-01T07:20:07.44214Z","shell.execute_reply.started":"2022-07-01T06:07:29.95371Z","shell.execute_reply":"2022-07-01T07:20:07.440528Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"========== fold: 0 training ==========\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [1][0/2500] Data 1.181 (1.181) Elapsed 0m 3s (remain 130m 34s) Loss: 5.3068(5.3068) Encoder Grad: 1.2318  Decoder Grad: 0.9010  \nEpoch: [1][250/2500] Data 0.000 (0.005) Elapsed 3m 36s (remain 32m 15s) Loss: 1.6891(2.4083) Encoder Grad: 0.2886  Decoder Grad: 0.8096  \nEpoch: [1][500/2500] Data 0.000 (0.003) Elapsed 7m 9s (remain 28m 32s) Loss: 1.5698(2.0120) Encoder Grad: 0.2327  Decoder Grad: 0.4803  \nEpoch: [1][750/2500] Data 0.000 (0.002) Elapsed 10m 40s (remain 24m 52s) Loss: 1.3957(1.8357) Encoder Grad: 0.2464  Decoder Grad: 0.3481  \nEpoch: [1][1000/2500] Data 0.000 (0.001) Elapsed 14m 11s (remain 21m 14s) Loss: 1.3128(1.7232) Encoder Grad: 0.2384  Decoder Grad: 0.3084  \nEpoch: [1][1250/2500] Data 0.000 (0.001) Elapsed 17m 43s (remain 17m 41s) Loss: 1.2978(1.6389) Encoder Grad: 0.4449  Decoder Grad: 0.8202  \nEpoch: [1][1500/2500] Data 0.000 (0.001) Elapsed 21m 16s (remain 14m 9s) Loss: 1.2395(1.5724) Encoder Grad: 0.2545  Decoder Grad: 0.5080  \nEpoch: [1][1750/2500] Data 0.000 (0.001) Elapsed 24m 48s (remain 10m 36s) Loss: 1.1390(1.5168) Encoder Grad: 0.4095  Decoder Grad: 0.8273  \nEpoch: [1][2000/2500] Data 0.000 (0.001) Elapsed 28m 21s (remain 7m 4s) Loss: 1.1131(1.4694) Encoder Grad: 0.2511  Decoder Grad: 0.4487  \nEpoch: [1][2250/2500] Data 0.000 (0.001) Elapsed 31m 54s (remain 3m 31s) Loss: 0.9877(1.4289) Encoder Grad: 0.3118  Decoder Grad: 0.4834  \nEpoch: [1][2499/2500] Data 0.000 (0.001) Elapsed 35m 27s (remain 0m 0s) Loss: 1.0559(1.3927) Encoder Grad: 0.2883  Decoder Grad: 0.3638  \nEVAL: [0/625] Data 1.059 (1.059) Elapsed 0m 1s (remain 19m 9s) \nEVAL: [250/625] Data 0.002 (0.007) Elapsed 1m 34s (remain 2m 20s) \nEVAL: [500/625] Data 0.003 (0.004) Elapsed 3m 5s (remain 0m 46s) \nEVAL: [624/625] Data 0.002 (0.004) Elapsed 3m 50s (remain 0m 0s) \n","output_type":"stream"},{"name":"stderr","text":"val labels: ['InChI=1S/C28H34O4/c1-4-25(31)10-7-21-8-12-27(20(3)13-21)28-15-26(11-5-19(28)2)32-18-22-6-9-23(16-29)24(14-22)17-30/h5-6,8-9,11-15,25,29-31H,4,7,10,16-18H2,1-3H3'\n 'InChI=1S/C16H14FN3O/c17-13-6-7-14(15(19)10-13)16(21)20(9-8-18)11-12-4-2-1-3-5-12/h1-7,10H,9,11,19H2'\n 'InChI=1S/C25H23N3O2S/c1-16(2)15-28-23(21-11-6-12-31-21)22(19-9-3-4-10-20(19)25(28)30)24(29)27-18-8-5-7-17(13-18)14-26/h3-13,16,22-23H,15H2,1-2H3,(H,27,29)'\n 'InChI=1S/C14H20N2O3/c1-3-4-7-16(9-13(15)18)14(19)12-6-5-11(17)8-10(12)2/h5-6,8,17H,3-4,7,9H2,1-2H3,(H2,15,18)'\n 'InChI=1S/C12H24N2O/c1-8(2)10-5-4-6-11(7-10)14-9(3)12(13)15/h8-11,14H,4-7H2,1-3H3,(H2,13,15)']\nval preds: ['InChI=1S/C29H36N2O3/c1-4-5-6-7-22-8-10-23(11-9-23)27(30)14-18-30(30(3)4)27(30)12-12-27(29)15-17-30(31)27-11-10-23(30)11-13-25/h6-8,12-16,20,23,28H,4-5,8-10,14-17,20-22H2,1-3H3', 'InChI=1S/C16H14FN3O/c17-13-5-3-4-12(6-11)14(19)9-16(19)18-10-12-5-2-6-16(17)10-14/h2-8,11,18H,9,14H2,1H3', 'InChI=1S/C24H22N3O2S/c1-16(2)14-24(27)27-20-9-5-6-15-25(20)27(29)27(28)25(28)18-8-4-5-12-23(18)29-22(29)12-12-23(27)29/h3-12,14,18H,12-13H2,1-2H3,(H,27,29)', 'InChI=1S/C15H22N2O3/c1-4-5-14(18)11-7-11(2)9-15(10-13)16(18)14(3)5-6-15/h6-7,10,14,18H,4-5,8-9,12H2,1-3H3,(H,16,18)', 'InChI=1S/C13H25N2O/c1-9(2)8-12(3)12(15)13-8-6-5-7-12(12)13(15)4/h8-10,13H,5-7H2,1-4H3,(H2,13,15)']\nEpoch 1 - avg_train_loss: 1.3927  time: 2359s\nEpoch 1 - avg_val_loss: 5.8060\nEpoch 1 - Score: 58.7385\nEpoch 1 - Save Best Score: 58.7385 Model\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [2][0/2500] Data 0.958 (0.958) Elapsed 0m 2s (remain 108m 11s) Loss: 1.0745(1.0745) Encoder Grad: 0.3609  Decoder Grad: 0.4302  \nEpoch: [2][250/2500] Data 0.000 (0.004) Elapsed 3m 36s (remain 32m 20s) Loss: 0.9904(1.0266) Encoder Grad: 0.3466  Decoder Grad: 0.5014  \nEpoch: [2][500/2500] Data 0.000 (0.002) Elapsed 7m 9s (remain 28m 35s) Loss: 0.9639(1.0133) Encoder Grad: 0.3999  Decoder Grad: 0.3937  \nEpoch: [2][750/2500] Data 0.000 (0.001) Elapsed 10m 40s (remain 24m 51s) Loss: 1.0008(0.9995) Encoder Grad: 0.3706  Decoder Grad: 0.3502  \nEpoch: [2][1000/2500] Data 0.000 (0.001) Elapsed 14m 13s (remain 21m 17s) Loss: 0.9111(0.9880) Encoder Grad: 0.3750  Decoder Grad: 0.4390  \nEpoch: [2][1250/2500] Data 0.000 (0.001) Elapsed 17m 45s (remain 17m 43s) Loss: 0.8804(0.9764) Encoder Grad: 0.3751  Decoder Grad: 0.3753  \nEpoch: [2][1500/2500] Data 0.000 (0.001) Elapsed 21m 18s (remain 14m 10s) Loss: 0.9569(0.9666) Encoder Grad: 0.4354  Decoder Grad: 0.5052  \nEpoch: [2][1750/2500] Data 0.000 (0.001) Elapsed 24m 50s (remain 10m 37s) Loss: 0.9764(0.9571) Encoder Grad: 0.4711  Decoder Grad: 0.4588  \nEpoch: [2][2000/2500] Data 0.000 (0.001) Elapsed 28m 21s (remain 7m 4s) Loss: 0.8660(0.9497) Encoder Grad: 0.3959  Decoder Grad: 0.3912  \nEpoch: [2][2250/2500] Data 0.000 (0.001) Elapsed 31m 52s (remain 3m 31s) Loss: 0.8767(0.9411) Encoder Grad: 0.3701  Decoder Grad: 0.4007  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-9c52255a89ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-ac4ff34f4e4a>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(folds, fold, encoder, decoder)\u001b[0m\n\u001b[1;32m    127\u001b[0m         avg_loss = train_fn(train_loader, encoder, decoder, criterion, \n\u001b[1;32m    128\u001b[0m                             \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                             encoder_scheduler, decoder_scheduler, device)\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-19a006d93d35>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch, encoder_scheduler, decoder_scheduler, device)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mtargets\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mcaps_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-f71cfc0dc480>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_out, encoded_captions, caption_lengths)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# set decode length by caption length - 1 because of omitting start token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mdecode_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcaption_lengths\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mpredictions\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0malphas\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"efficient_net_b1 = [98.8575, 73.6695, 72.2025, 75.6785, 72.8615, 73.5025, 67.227, 74.857, 65.8755, 63.5605, 62.218, 61.8435, 62.494, 62.4595, 60.855, 61.2445, 60.16, 59.3595, 58.445, 57.9975]\nefficient_net_b1_loss = [2.3931803798675535, 1.628167809009552, 1.5203406267166137, 1.4713778057098388, 1.4598911304473876, 1.4517746849060058, 1.4298730211257935, 1.3751705694198608, 1.304869324684143, 1.237863293170929, 1.1833606758117676, 1.145661545753479, 1.1337855563163757, 1.132903272151947, 1.136840611934662, 1.1170298233032228, 1.099350608587265, 1.0599301619529724, 1.0109912855625152, 0.9796263043880462]\n\nb2_train_score = [113.59625, 98.33, 96.615875, 94.968375, 94.140625, 94.623625, 93.863125, 92.865125, 91.84775, 91.042375]\nefficient_net_b2 = [134.222, 74.5255, 80.9045, 76.0485, 74.751, 71.3455, 73.473, 66.329, 65.993, 64.856, 62.6475, 60.817, 61.522, 60.7185, 61.715, 61.191, 59.6985, 58.849, 59.1755, 56.8945]\nefficient_net_b2_loss = [2.415083875179291, 1.6237234082221985, 1.5118725147247314, 1.4611049151420594, 1.4461445527076722, 1.4379349818229676, 1.409820526123047, 1.3576485114097596, 1.289951596736908, 1.2252001495361329, 1.1670255136489869, 1.1304647407531738, 1.1173807997703553, 1.1154722766876222, 1.1165171647071839, 1.1063506488800048, 1.0751800196170807, 1.0316696577072144, 0.9882933089733124, 0.9550682859420776]\nb2_val_loss = [5.1344, 5.6729, 6.0253, 6.0020, 6.0312, 6.1491, 6.1898, 6.3044, 6.4776, 6.3926]\n\nefficient_net_b3 = [95.667, 83.3315, 76.262, 73.2065, 73.8655, 73.544, 67.0345, 68.6645, 67.535, 61.066, 62.0325, 59.302, 59.429, 58.9515, 60.4385, 62.1805, 60.259, 56.178, 55.834, 55.59]\nefficient_net_b3_loss = [2.3996424560546874, 1.626395184993744, 1.5165460896492005, 1.465637098789215, 1.4504963603019714, 1.443153549194336, 1.4172072825431823, 1.359358612060547, 1.286353558063507, 1.2174233260154723, 1.1592508525848388, 1.1197750024795532, 1.107436776638031, 1.1040277109146117, 1.1063713212013244, 1.096435488462448, 1.0655429661273956, 1.018177206516266, 0.9751036190986633, 0.9428163130283356]\nefficient_net_b4 = [250.2225, 229.892, 111.1725, 133.9945, 134.934, 146.641, 114.107, 81.764, 79.7775, 79.208, 80.16, 74.4985, 75.5305, 74.5765, 75.9875, 74.524, 74.388, 70.883, 72.6915, 68.602]\nefficient_net_b4_loss = [3.1840683708190918, 2.869856611251831, 2.4396746187210083, 2.160268536567688, 2.102885934829712, 2.0599588737487795, 1.903951898097992, 1.718906756401062, 1.5983499155044556, 1.5180621557235718, 1.4636279873847962, 1.4322679510116578, 1.423622896194458, 1.41957506275177, 1.4109104981422425, 1.3853939290046693, 1.350641189098358, 1.3104349522590637, 1.2739801769256591, 1.2522076315879822]\nefficient_net_b5 = [83.609, 74.229, 74.75, 76.512, 74.7315, 73.3395, 72.2375, 82.597, 83.251, 76.9785, 76.961, 75.74, 74.002, 75.263, 70.516, 72.561, 77.1875, 70.863, 75.4075, 74.069]\nefficient_net_b5_loss = [2.0829786279201508, 1.557403645992279, 1.453078752040863, 1.401730174779892, 1.3860347771644592, 1.3810959169864654, 1.3654353585243224, 1.329354391336441, 1.2842514560222626, 1.2379606404304504, 1.194757817029953, 1.1652341079711914, 1.1550245118141174, 1.1558650314807892, 1.162438308238983, 1.1578786492347717, 1.1389484195709227, 1.1131686375141143, 1.0777646358013153, 1.0513258594274522]\nefficient_net_b6 = [130.5265, 86.1965, 88.264, 88.764, 80.958, 85.9995, 90.244, 97.253, 78.8195, 79.524, 98.389, 92.6205, 93.9555, 90.7505, 81.7735, 80.19, 71.238, 74.874, 72.5205, 72.3895]\nefficient_net_b6_loss = [2.0776397485733034, 1.5548398282527924, 1.4560341963768004, 1.4082622706890107, 1.3940581126213074, 1.3898768017292022, 1.3772511410713195, 1.3386958220005036, 1.2897724032402038, 1.2428567588329316, 1.2017232131958009, 1.1723768224716187, 1.1614158926010132, 1.1626962089538575, 1.1686801233291626, 1.1650111904144287, 1.1489011342525481, 1.1179789177179336, 1.0860439949035645, 1.0597657492160797]\n\nmobile_net = [255.6650, 78.9660, 70.1145, 77.7875, 74.6280, 71.4225, 73.6685, 70.1670, 71.3795, 74.1130]\ntnt = [98.3080, 88.8265, 79.2645, 70.0020, 72.5620, 71.3095, 70.1475, 78.2650, 69.4255, 69.5965]\nvit = [108.6810, 74.8460, 72.3420, 76.8190, 70.9615, 70.2555, 69.4180, 72.0445, 71.0110, 69.7120]\nresnet50 = [164.9995, 100.7000, 83.7735, 70.8685, 70.8185, 70.9180, 70.0885, 73.7620, 70.0250, 71.5485]\n\n# 100000\ntrain_losses = [1.3926620348930359, 0.9320967340946198, 0.7832191390752792, 0.7080147546768188, 0.6866521205425262, 0.6801256247997284, 0.6680761493682862, 0.6320035829424858, 0.5764910193800926, 0.5121954302072526]\nval_loss = [5.8059, 5.5785, 5.6106, 5.5010, 5.4806, 5.4974, 5.4568, 5.3981, 5.1789, 4.9673]\nvalidation_scores = [58.73845, 53.42155, 49.25315, 47.46995, 46.9863, 46.3712, 45.1068, 42.47475, 39.59625, 36.06345]","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:20:07.443474Z","iopub.status.idle":"2022-07-01T07:20:07.444241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = range(1, 21)\nx = range(1, 11)\nplt.plot(x, efficient_net_b1[:10])\nplt.plot(x, mobile_net)\nplt.plot(x, resnet50)\nplt.plot(x, tnt)\nplt.plot(x, vit)\nplt.ylabel(\"Levenshtein Distance\")\nplt.xlabel(\"Epochs\")\nplt.ylim(50, 100)\nplt.legend(['EfficientNet B1', 'MobileNet', 'ResNet50', 'TNT', 'ViT'])\nplt.savefig(\"fig2.jpg\", dpi=200)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:20:07.445436Z","iopub.status.idle":"2022-07-01T07:20:07.446186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.plot(x1, efficient_net_b1)\nplt.plot(x1, efficient_net_b2)\nplt.plot(x1, efficient_net_b3)\nplt.plot(x1, efficient_net_b4)\nplt.plot(x1, efficient_net_b5)\nplt.plot(x1, efficient_net_b6)\n\nplt.ylabel(\"Levenshtein Distance\")\nplt.xlabel(\"Epochs\")\nplt.xticks(range(2, 22, 2))\nplt.ylim(50, 100)\nplt.legend(['EfficientNet B1', 'EfficientNet B2', 'EfficientNet B3', 'EfficientNet B4', 'EfficientNet B5', 'EfficientNet B6'])\nplt.savefig(\"fig4.jpg\", dpi=200)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:20:07.447271Z","iopub.status.idle":"2022-07-01T07:20:07.448017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.plot(x1, efficient_net_b1_loss)\nplt.plot(x1, efficient_net_b2_loss)\nplt.plot(x1, efficient_net_b3_loss)\nplt.plot(x1, efficient_net_b4_loss)\nplt.plot(x1, efficient_net_b5_loss)\nplt.plot(x1, efficient_net_b6_loss)\n\nplt.ylabel(\"Cross Entropy Loss\")\nplt.xlabel(\"Epochs\")\nplt.xticks(range(2, 22, 2))\nplt.ylim(0.8, 1.5)\nplt.legend(['EfficientNet B1', 'EfficientNet B2', 'EfficientNet B3', 'EfficientNet B4', 'EfficientNet B5', 'EfficientNet B6'])\nplt.savefig(\"fig6.jpg\", dpi=200)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:20:07.449103Z","iopub.status.idle":"2022-07-01T07:20:07.449856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission (do not run the cells below)","metadata":{}},{"cell_type":"code","source":"# def inference(test_loader, encoder, decoder, tokenizer, device):\n    \n#     encoder.eval()\n#     decoder.eval()\n    \n#     text_preds = []\n#     tk0 = tqdm(test_loader, total = len(test_loader))\n    \n#     for images in tk0:\n        \n#         images = images.to(device)\n        \n#         with torch.no_grad():\n#             features = encoder(images)\n#             predictions = decoder.predict(features, CFG.max_len, tokenizer)\n            \n#         predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n#         _text_preds = tokenizer.predict_captions(predicted_sequence)\n#         text_preds.append(_text_preds)\n        \n#     text_preds = np.concatenate(text_preds)\n    \n#     return text_preds","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:20:07.450967Z","iopub.status.idle":"2022-07-01T07:20:07.451726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\n\n# test_dataset = TestDataset(test, transform = get_transforms(data = 'valid'))\n# test_loader  = DataLoader(test_dataset, batch_size = 256, shuffle = False, num_workers = CFG.num_workers)\n# predictions  = inference(test_loader, encoder, decoder, tokenizer, device)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:20:07.452812Z","iopub.status.idle":"2022-07-01T07:20:07.453548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n#  submission\n# ====================================================\n\n# test['InChI'] = [f\"InChI=1S/{text}\" for text in predictions]\n# test[['image_id', 'InChI']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:20:07.454643Z","iopub.status.idle":"2022-07-01T07:20:07.455367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \n#  validation loss \ntrain_loss = [2.415083875179291, 1.6237234082221985, 1.5118725147247314, 1.4611049151420594, 1.4461445527076722, 1.4379349818229676, 1.409820526123047, 1.3576485114097596, 1.289951596736908, 1.2252001495361329, 1.1670255136489869, 1.1304647407531738, 1.1173807997703553, 1.1154722766876222, 1.1165171647071839, 1.1063506488800048, 1.0751800196170807, 1.0316696577072144, 0.9882933089733124, 0.9550682859420776]\nvalidation_loss = [5.1344, 5.6729, 6.0253, 6.0020, 6.0312, 6.1491, 6.1898, 6.3044, 6.4776, 6.3926]\nvalidation_score = [134.222, 74.5255, 80.9045, 76.0485, 74.751, 71.3455, 73.473, 66.329, 65.993, 64.856, 62.6475, 60.817, 61.522, 60.7185, 61.715, 61.191, 59.6985, 58.849, 59.1755, 56.8945]\n# validation Levenshtein distance score epochs modeltraining   validation lossepochs \n# validation loss  valid_fn():\n# predictions = decoder.predict(features, CFG.max_len, tokenizer)\n# decode_lengths = (label_lengths.squeeze(1) - 1).tolist()\n# targets = labels[:, 1:]\n# predictions = pack_padded_sequence(predictions, decode_lengths, batch_first=True, enforce_sorted=False).data\n# targets     = pack_padded_sequence(targets, decode_lengths, batch_first=True, enforce_sorted=False).data\n# loss        = criterion(predictions, targets)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:20:07.456494Z","iopub.status.idle":"2022-07-01T07:20:07.457228Z"},"trusted":true},"execution_count":null,"outputs":[]}]}